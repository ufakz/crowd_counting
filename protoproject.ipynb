{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7b60138-e38d-4d28-aada-3220607b695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage import measure\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import tracemalloc\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2766e5-716d-425c-ab0d-32db4abe9115",
   "metadata": {},
   "source": [
    "## Preprocess Image\n",
    "---\n",
    "1. Loads the image directly in grayscale. It is a simple and fast conversion\n",
    "2. Enhances the local contrast in the image:\n",
    "> Using a medium-size tileGridSize for a contrast that is neither too local nor too global.\n",
    "> Using a medium-size clipLimit to increase the contrast, but it is not a high value so as not to cause excessive noise, but keep in mind that some people can be seen incompletely or far away, so a low value does not work.\n",
    "\n",
    "3. Apply Gaussian: We want to reduce noise, but since the people are small silhouettes, we don't want their details to disappear, so we set the kernel smaller. The standard deviation is set to 0 for automatic adjustment. This setting consists of OpenCV calculating a value based on the kernel size, balancing smoothing and detail preservation.\n",
    "4. Maintain detail in darker areas while controlling the effect of the overexposed regions. thresh 127 is balanced, if it is smaller it causes overexposure, and if it is larger it causes more “noise”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30af7461-1a33-4cd1-b787-4495604ba3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_name, show_image=False):\n",
    "    image_path=f\"data/{image_name}\"\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:\n",
    "        print(f\"The image cannot be loaded: {image_path}\")\n",
    "        return None\n",
    "    if show_image:\n",
    "        plt.imshow(image_path)\n",
    "    return image\n",
    "\n",
    "def clahe(image: np.ndarray,show_image=False, clip_limit=20.0, grid_size=(14, 14)) -> np.ndarray:\n",
    "    \"\"\"Contrast-limited adaptive histogram equalization.\"\"\"\n",
    "    image = image.astype('uint8')   # Ensure that the image is of type uint8\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=grid_size)\n",
    "    if show_image:\n",
    "        plt.title('Clahe')\n",
    "        plt.imshow(clahe)\n",
    "    return clahe.apply(image)\n",
    "\n",
    "def preprocess_image(image_name,show_image=False):\n",
    "    image_grayscale = load_image(image_name,show_image)\n",
    "    if image_grayscale is None:\n",
    "        return None\n",
    "\n",
    "    img_equalized = clahe(image_grayscale,show_image)\n",
    "    blurred_image = cv2.GaussianBlur(img_equalized, (3, 3), 0)\n",
    "    _, binary_image = cv2.threshold(blurred_image, 127, 255, cv2.THRESH_TRUNC)\n",
    "    return binary_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864045cf-cf01-45a6-b9cc-b5b52ecfc980",
   "metadata": {},
   "source": [
    "### Performance\n",
    "---\n",
    "\n",
    "To evaluate the algorithm using [wrapper](https://stackoverflow.com/questions/36610950/passing-kwargs-received-in-a-wrapper-function-definition-to-arguments-of-an-e).  \n",
    "- tracemalloc to monitor memory.\n",
    "- time for time\n",
    "- os for CPU cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db701a8c-9805-4c86-a2c9-05ee42a4585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_performance(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        tracemalloc.start()\n",
    "        start_time = time.perf_counter()\n",
    "        start_cpu = os.times().user\n",
    "        result = func(*args, **kwargs)\n",
    "        end_cpu = os.times().user\n",
    "        end_time = time.perf_counter()\n",
    "        current, peak = tracemalloc.get_traced_memory()\n",
    "        tracemalloc.stop()\n",
    "        print(f\"Function {func.__name__} took {end_time - start_time:.4f} seconds\")\n",
    "        print(f\"CPU cycles used: {end_cpu - start_cpu:.4f}\")\n",
    "        print(f\"Current memory usage is {current / 10**6:.4f} MB; Peak was {peak / 10**6:.4f} MB\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0522466-4f6d-4a3e-85e5-4750fdd94dda",
   "metadata": {},
   "source": [
    "## Extract differences\n",
    "---\n",
    "1. The XOR operator will highlight the differences between the two images, which in this case will be the additional objects on the background\n",
    "2. Apply opening to remove small noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d296b064-cad3-428d-93f1-b13487e3b094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation_foreground(binary_image: np.ndarray, binary_bckg_image: np.ndarray,show_image=False):\n",
    "    segmented_image = cv2.bitwise_xor(binary_bckg_image, binary_image)\n",
    "    # Delete small differences\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    refined_image = cv2.morphologyEx(segmented_image, cv2.MORPH_OPEN, kernel)\n",
    "    return refined_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f13b7e6-d865-4ed4-945c-4ad18708e92c",
   "metadata": {},
   "source": [
    "## Detect edges\n",
    "---\n",
    "1. Detect edges and using \"adhoc\" numbers\n",
    "2. Enhance edges improve to separate them from the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0e46bdc-427d-4364-a15e-7f22f3de4c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_detection(image: np.ndarray,show_image=False, min_threshold=230, max_threshold=180):\n",
    "    edges = cv2.Canny(image, min_threshold, max_threshold)\n",
    "    intensified_edges = cv2.Laplacian(edges, cv2.CV_64F)\n",
    "    return intensified_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7c8171-e636-47c2-8816-c91e2a07c21e",
   "metadata": {},
   "source": [
    "## Obtain person detected coordinates\n",
    "---\n",
    "1. Convert image from float64 to 8uint and 1 channel to be used by findContours\n",
    "2. Obtain contours, and filter by the perimeter size\n",
    "3. Obtain the center points of the contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afed2834-1cea-4853-8a04-740668d5f479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_contours(image: np.ndarray,show_image=False):\n",
    "    image_8b_c1 = cv2.convertScaleAbs(image, alpha=(255.0/np.max(image)))\n",
    "    contours, _ = cv2.findContours(image_8b_c1, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    min_perimeter = 50\n",
    "    filtered_contours = [cnt for cnt in contours if cv2.arcLength(cnt, True) > min_perimeter]\n",
    "    return filtered_contours\n",
    "\n",
    "def obtain_center_points(filtered_contours,show_image=False):\n",
    "    y_min = 420\n",
    "    contour_points = []\n",
    "    for contorno in filtered_contours:\n",
    "        M = cv2.moments(contorno)\n",
    "        if M['m00'] != 0:\n",
    "            cx = int(M['m10'] / M['m00'])\n",
    "            cy = int(M['m01'] / M['m00'])\n",
    "            if cy > y_min:\n",
    "                contour_points.append((cx, cy))\n",
    "    return contour_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f810f80-26f6-4985-b5f0-a063db2d98a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc814355-f37f-43b8-9f39-10a7297c9174",
   "metadata": {},
   "outputs": [],
   "source": [
    "@measure_performance\n",
    "def process_image(image_path, bckg_image,show_image=False):\n",
    "    binary_image = preprocess_image(image_path)\n",
    "    if binary_image is None:\n",
    "        return None\n",
    "\n",
    "    segmented_image = segmentation_foreground(binary_image, bckg_image,show_image)\n",
    "    edges = edge_detection(segmented_image,show_image)\n",
    "    contours = obtain_contours(edges,show_image)\n",
    "    center_points = obtain_center_points(contours,show_image)\n",
    "    return {\n",
    "        'image': image_path,\n",
    "        'count': len(center_points),\n",
    "        'points': center_points\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62aeccdc-7854-43c4-9f4b-6e03a9f63dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(image_names, bckg_image,show_image=False):\n",
    "    processed_images = []\n",
    "    for name in image_names:\n",
    "        processed_image = process_image(name, bckg_image,show_image)\n",
    "        if processed_image is not None:\n",
    "            processed_images.append(processed_image)\n",
    "    return processed_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1a8900-9adc-4e9a-9c42-efc2b4c794b3",
   "metadata": {},
   "source": [
    "## Obtain ground truth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e7b2d12-cb6e-41cb-9311-0908d9ff65b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_manual_annotations(input_file,with_headers=False):\n",
    "    file_path=f\"data/{input_file}\"\n",
    "    # Diccionario para almacenar los datos\n",
    "    data = defaultdict(lambda: {'people_count': 0, 'coordinates': []})\n",
    "    # No headers\n",
    "    with open(file_path, 'r', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            if with_headers:\n",
    "                name = row['name']\n",
    "                x = int(row['x'])\n",
    "                y = int(row['y'])\n",
    "            else:  \n",
    "                # Extract values based on the order: label, x, y, name, height, width\n",
    "                name = row[3]\n",
    "                x = int(row[1])\n",
    "                y = int(row[2])\n",
    "            data[name]['people_count'] += 1\n",
    "            data[name]['coordinates'].append((x, y))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e6f951-07e9-4fb7-a1c0-37d29b77bbda",
   "metadata": {},
   "source": [
    "## Validation\n",
    "---\n",
    "True Positives (TP):\n",
    "- A reference point (manual) is considered correctly detected (TP) if there is at least one detection point at a distance of 30 pixels on the Y-axis and 12 pixels on the X-axis.\n",
    "- Only the detection point closest to the reference point is counted as a TP to avoid inflating the number of TPs.\n",
    "\n",
    "False Negatives (FN):\n",
    "- A fiducial point is considered not detected (FN) if there is no detection point within the limits of 30 pixels on the Y-axis and 12 pixels on the X-axis.\n",
    "\n",
    "False Positive (FP):\n",
    "- A detection point is considered a false positive (FP) if it is not within the limits of 30 pixels on the Y-axis and 12 pixels on the X-axis of any reference point.\n",
    "\n",
    "Calculation Steps:\n",
    "- Initialization: point lists are converted to Numpy arrays and counters for TP, FP and FN are initialized.\n",
    "- Manual Point Traversal: For each manual point, the closest detection point that meets the distance thresholds in X and Y is searched.\n",
    "- Closeness Check: If a close detection point is found, the TP counter is incremented and that detection point is marked as used.\n",
    "- False Negative Detection: If no nearby detection point is found, the FN counter is incremented.\n",
    "- False Positive Calculation: All unused detection points are counted as FP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6c08d2-8c4f-4a14-8e2c-01b29cf0438b",
   "metadata": {},
   "source": [
    "True Positives (TP):  \n",
    "- A detected point is considered a true positive if it is close enough to a manual point. In this case, “close enough” means that the distance on the y-axis is less than or equal to 30 pixels and the distance on the x-axis is less than or equal to 12 pixels. If a detected point meets these criteria for a manual point, it is counted as a TP.\n",
    "\n",
    "False Negatives (FN):\n",
    "- A manual point is considered a false negative if no detected point is close enough to it (according to the criteria mentioned above). This means that the algorithm did not detect a person who was present according to the manual count.\n",
    "\n",
    "False Positives (FP):\n",
    "- A detected point is considered a false positive if it has not been matched with any manual point. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "535347e8-f4e2-4a3a-ad82-e6008c6d1a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tp_fp_fn(manual_points, detected_points, y_thresh=30, x_thresh=12):\n",
    "    manual_points = np.array(manual_points)\n",
    "    detected_points = np.array(detected_points)\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    detected_used = np.zeros(len(detected_points), dtype=bool)\n",
    "    for manual_point in manual_points:\n",
    "        detected = False\n",
    "        closest_index = None\n",
    "        closest_distance = float('inf')\n",
    "        \n",
    "        for i, detected_point in enumerate(detected_points):\n",
    "            if not detected_used[i]:\n",
    "                distance_y = abs(manual_point[1] - detected_point[1])\n",
    "                distance_x = abs(manual_point[0] - detected_point[0])\n",
    "                if distance_y <= y_thresh and distance_x <= x_thresh:\n",
    "                    distance = np.sqrt(distance_y ** 2 + distance_x ** 2)\n",
    "                    if distance < closest_distance:\n",
    "                        closest_distance = distance\n",
    "                        closest_index = i\n",
    "                        detected = True\n",
    "        \n",
    "        if detected and closest_index is not None:\n",
    "            tp += 1\n",
    "            detected_used[closest_index] = True\n",
    "        else:\n",
    "            fn += 1\n",
    "\n",
    "    fp = np.sum(~detected_used)\n",
    "\n",
    "    return tp, fp, fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4ade52-baff-4042-8cef-5013105fdc99",
   "metadata": {},
   "source": [
    "Precision: It is the proportion of true positives (TP) over the total positive predictions (TP + FP).  \n",
    "Recall: It is the proportion of true positives over the total number of instances that are truly positive (TP + FN).  \n",
    "F1 Score: It is the harmonic mean of accuracy and completeness, and is calculated as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65f97207-a583-47f7-8ce2-ea151d4d2882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_F1score_recall_precision(manual_points, detected_points):\n",
    "    tp, fp, fn=calculate_tp_fp_fn(manual_points,detected_points)\n",
    "    precision=tp/(tp+fp)\n",
    "    recall=tp/(tp+fn)\n",
    "    f1score=2*((precision*recall)/(precision+recall))\n",
    "    return precision, recall, f1score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b6481f0-b3fb-44a0-adb3-74696578153e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(manual_results, results):\n",
    "    comparison = []\n",
    "    for result in results:\n",
    "        image_name = result['image']\n",
    "        manual_data = manual_results.get(image_name, {'people_count': 0, 'coordinates': []})\n",
    "        manual_count = manual_data['people_count']\n",
    "        manual_points = manual_data['coordinates']\n",
    "        detected_count = result['count']\n",
    "        detected_points = result['points']\n",
    "        precision, recall, f1score=calculate_F1score_recall_precision(manual_points,detected_points)\n",
    "        mse=np.mean((manual_count - detected_count) ** 2)#sustraendo debe ser la predicción\n",
    "        comparison.append({\n",
    "            'image': image_name,\n",
    "            'manual_count': manual_count,\n",
    "            'detected_count': detected_count,\n",
    "            'MSE':mse,\n",
    "            'recall':recall,\n",
    "            'precision':precision,\n",
    "            'f1score': f1score\n",
    "        })\n",
    "    return comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84db8d05-6140-47ba-ae75-aae8c92df9c9",
   "metadata": {},
   "source": [
    "### MAIN\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c44bc6f-a40b-4811-950f-53316baeec38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function process_image took 0.0295 seconds\n",
      "CPU cycles used: 0.0400\n",
      "Current memory usage is 0.0075 MB; Peak was 23.0908 MB\n",
      "Function process_image took 0.0199 seconds\n",
      "CPU cycles used: 0.0200\n",
      "Current memory usage is 0.0089 MB; Peak was 23.1816 MB\n",
      "Function process_image took 0.0239 seconds\n",
      "CPU cycles used: 0.0400\n",
      "Current memory usage is 0.0131 MB; Peak was 23.2914 MB\n",
      "Image: 1660287600.jpg, Manual Count: 19, Detected Count: 107\n",
      " MSE Image level: 7744.0, Recall: 0.42105263157894735, precision: 0.07476635514018691 f1score: 0.12698412698412698\n",
      "Image: 1660294800.jpg, Manual Count: 66, Detected Count: 139\n",
      " MSE Image level: 5329.0, Recall: 0.22727272727272727, precision: 0.1079136690647482 f1score: 0.1463414634146341\n",
      "Image: 1660320000.jpg, Manual Count: 155, Detected Count: 218\n",
      " MSE Image level: 3969.0, Recall: 0.44516129032258067, precision: 0.3165137614678899 f1score: 0.36997319034852544\n"
     ]
    }
   ],
   "source": [
    "manual_path = \"manual_alt.csv\"\n",
    "manual_results = read_manual_annotations(manual_path)\n",
    "bckg_image_name = '1660284000.jpg'\n",
    "bckg_image = preprocess_image(bckg_image_name)\n",
    "image_names = [\"1660287600.jpg\", \"1660294800.jpg\",\"1660320000.jpg\"]\n",
    "\n",
    "results = process_images(image_names, bckg_image)\n",
    "comparison = validation(manual_results, results)\n",
    "\n",
    "for comp in comparison:\n",
    "    print(f\"Image: {comp['image']}, Manual Count: {comp['manual_count']}, Detected Count: {comp['detected_count']}\")\n",
    "    print(f\" MSE Image level: {comp['MSE']}, Recall: {comp['recall']}, precision: {comp['precision']} f1score: {comp['f1score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d329b69-b6bb-4291-b227-6ea91aad5fae",
   "metadata": {},
   "source": [
    "Notes:  \n",
    "- Currently, the background image keeps with the 2 persons.\n",
    "- They are not equally enlightened in the same way; however, on the test done, the contrast is lowered (in the same image).\n",
    "- Not establishing specific contours for a person (different cases).\n",
    ". How do you achieve accuracy with the points? Establish a certain distance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53adc963-e38f-4fb6-abfb-fc76c18a69bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6b4637-0c48-43f7-8ab5-68edbc0965a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-utu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
