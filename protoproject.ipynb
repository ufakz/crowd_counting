{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7b60138-e38d-4d28-aada-3220607b695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage import measure\n",
    "import csv\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2766e5-716d-425c-ab0d-32db4abe9115",
   "metadata": {},
   "source": [
    "## Preprocess Image\n",
    "---\n",
    "1. Loads the image directly in grayscale. It is a simple and fast conversion\n",
    "2. Enhances the local contrast in the image:\n",
    "> Using a medium-size tileGridSize for a contrast that is neither too local nor too global.\n",
    "> Using a medium-size clipLimit to increase the contrast, but it is not a high value so as not to cause excessive noise, but keep in mind that some people can be seen incompletely or far away, so a low value does not work.\n",
    "\n",
    "3. Apply Gaussian: We want to reduce noise, but since the people are small silhouettes, we don't want their details to disappear, so we set the kernel smaller. The standard deviation is set to 0 for automatic adjustment. This setting consists of OpenCV calculating a value based on the kernel size, balancing smoothing and detail preservation.\n",
    "4. Maintain detail in darker areas while controlling the effect of the overexposed regions. thresh 127 is balanced, if it is smaller it causes overexposure, and if it is larger it causes more “noise”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30af7461-1a33-4cd1-b787-4495604ba3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_name, show_image=False):\n",
    "    image_path=f\"data/{image_name}\"\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:\n",
    "        print(f\"The image cannot be loaded: {image_path}\")\n",
    "        return None\n",
    "    if show_image:\n",
    "        plt.imshow(image_path)\n",
    "    return image\n",
    "\n",
    "def clahe(image: np.ndarray,show_image=False, clip_limit=20.0, grid_size=(14, 14)) -> np.ndarray:\n",
    "    \"\"\"Contrast-limited adaptive histogram equalization.\"\"\"\n",
    "    image = image.astype('uint8')   # Ensure that the image is of type uint8\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=grid_size)\n",
    "    if show_image:\n",
    "        plt.title('Clahe')\n",
    "        plt.imshow(clahe)\n",
    "    return clahe.apply(image)\n",
    "\n",
    "def preprocess_image(image_name,show_image=False):\n",
    "    image_grayscale = load_image(image_name,show_image)\n",
    "    if image_grayscale is None:\n",
    "        return None\n",
    "\n",
    "    img_equalized = clahe(image_grayscale,show_image)\n",
    "    blurred_image = cv2.GaussianBlur(img_equalized, (3, 3), 0)\n",
    "    _, binary_image = cv2.threshold(blurred_image, 127, 255, cv2.THRESH_TRUNC)\n",
    "    return binary_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0522466-4f6d-4a3e-85e5-4750fdd94dda",
   "metadata": {},
   "source": [
    "## Extract differences\n",
    "---\n",
    "1. The XOR operator will highlight the differences between the two images, which in this case will be the additional objects on the background\n",
    "2. Apply opening to remove small noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d296b064-cad3-428d-93f1-b13487e3b094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation_foreground(binary_image: np.ndarray, binary_bckg_image: np.ndarray,show_image=False):\n",
    "    segmented_image = cv2.bitwise_xor(binary_bckg_image, binary_image)\n",
    "    # Delete small differences\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    refined_image = cv2.morphologyEx(segmented_image, cv2.MORPH_OPEN, kernel)\n",
    "    return refined_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f13b7e6-d865-4ed4-945c-4ad18708e92c",
   "metadata": {},
   "source": [
    "## Detect edges\n",
    "---\n",
    "1. Detect edges and using \"adhoc\" numbers\n",
    "2. Enhance edges improve to separate them from the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0e46bdc-427d-4364-a15e-7f22f3de4c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_detection(image: np.ndarray,show_image=False, min_threshold=230, max_threshold=180):\n",
    "    edges = cv2.Canny(image, min_threshold, max_threshold)\n",
    "    intensified_edges = cv2.Laplacian(edges, cv2.CV_64F)\n",
    "    return intensified_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7c8171-e636-47c2-8816-c91e2a07c21e",
   "metadata": {},
   "source": [
    "## Obtain person detected coordinates\n",
    "---\n",
    "1. Convert image from float64 to 8uint and 1 channel to be used by findContours\n",
    "2. Obtain contours, and filter by the perimeter size\n",
    "3. Obtain the center points of the contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afed2834-1cea-4853-8a04-740668d5f479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_contours(image: np.ndarray,show_image=False):\n",
    "    image_8b_c1 = cv2.convertScaleAbs(image, alpha=(255.0/np.max(image)))\n",
    "    contours, _ = cv2.findContours(image_8b_c1, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    min_perimeter = 50\n",
    "    filtered_contours = [cnt for cnt in contours if cv2.arcLength(cnt, True) > min_perimeter]\n",
    "    return filtered_contours\n",
    "def obtain_center_points(filtered_contours,show_image=False):\n",
    "    y_min = 420\n",
    "    contour_points = []\n",
    "    for contorno in filtered_contours:\n",
    "        M = cv2.moments(contorno)\n",
    "        if M['m00'] != 0:\n",
    "            cx = int(M['m10'] / M['m00'])\n",
    "            cy = int(M['m01'] / M['m00'])\n",
    "            if cy > y_min:\n",
    "                contour_points.append((cx, cy))\n",
    "    return contour_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f810f80-26f6-4985-b5f0-a063db2d98a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc814355-f37f-43b8-9f39-10a7297c9174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image_path, bckg_image,show_image=False):\n",
    "    binary_image = preprocess_image(image_path)\n",
    "    if binary_image is None:\n",
    "        return None\n",
    "\n",
    "    segmented_image = segmentation_foreground(binary_image, bckg_image,show_image)\n",
    "    edges = edge_detection(segmented_image,show_image)\n",
    "    contours = obtain_contours(edges,show_image)\n",
    "    center_points = obtain_center_points(contours,show_image)\n",
    "    return {\n",
    "        'image': image_path,\n",
    "        'count': len(center_points),\n",
    "        'points': center_points\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62aeccdc-7854-43c4-9f4b-6e03a9f63dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(image_names, bckg_image,show_image=False):\n",
    "    processed_images = []\n",
    "    for name in image_names:\n",
    "        processed_image = process_image(name, bckg_image,show_image)\n",
    "        if processed_image is not None:\n",
    "            processed_images.append(processed_image)\n",
    "    return processed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff83466-e423-4831-9ce8-0aa189c5278f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e7b2d12-cb6e-41cb-9311-0908d9ff65b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_manual_annotations(input_file,with_headers=False):\n",
    "    file_path=f\"data/{input_file}\"\n",
    "    # Diccionario para almacenar los datos\n",
    "    data = defaultdict(lambda: {'people_count': 0, 'coordinates': []})\n",
    "    # No headers\n",
    "    with open(file_path, 'r', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            if with_headers:\n",
    "                name = row['name']\n",
    "                x = int(row['x'])\n",
    "                y = int(row['y'])\n",
    "            else:  \n",
    "                # Extract values based on the order: label, x, y, name, height, width\n",
    "                name = row[3]\n",
    "                x = int(row[1])\n",
    "                y = int(row[2])\n",
    "            data[name]['people_count'] += 1\n",
    "            data[name]['coordinates'].append((x, y))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7444c0-75b2-47a6-a24b-5eb3641b78a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b6481f0-b3fb-44a0-adb3-74696578153e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(manual_results, results):\n",
    "    comparison = []\n",
    "    for result in results:\n",
    "        image_name = result['image']\n",
    "        manual_data = manual_results.get(image_name, {'people_count': 0, 'coordinates': []})\n",
    "        manual_count = manual_data['people_count']\n",
    "        manual_points = manual_data['coordinates']\n",
    "        detected_count = result['count']\n",
    "        detected_points = result['points']\n",
    "        #Has to imrpove to set a proper distance, and cases where multiple prediction dots are on the same person\n",
    "        close_points = sum(1 for dp in detected_points if any(np.linalg.norm(np.array(dp) - np.array(mp)) < 20 for mp in manual_points))\n",
    "        comparison.append({\n",
    "            'image': image_name,\n",
    "            'manual_count': manual_count,\n",
    "            'detected_count': detected_count,\n",
    "            'close_points': close_points\n",
    "        })\n",
    "    return comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84db8d05-6140-47ba-ae75-aae8c92df9c9",
   "metadata": {},
   "source": [
    "### Validation\n",
    "---\n",
    "**Image level**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c44bc6f-a40b-4811-950f-53316baeec38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 1660287600.jpg, Manual Count: 19, Detected Count: 107, MSE Image level: 7744.0, Close Points: 8\n",
      "Image: 1660294800.jpg, Manual Count: 66, Detected Count: 139, MSE Image level: 5329.0, Close Points: 16\n",
      "Image: 1660320000.jpg, Manual Count: 155, Detected Count: 218, MSE Image level: 3969.0, Close Points: 66\n"
     ]
    }
   ],
   "source": [
    "manual_path = \"manual_annotations.csv\"\n",
    "manual_results = read_manual_annotations(manual_path)\n",
    "bckg_image_name = '1660284000.jpg'\n",
    "bckg_image = preprocess_image(bckg_image_name)\n",
    "image_names = [\"1660287600.jpg\", \"1660294800.jpg\",\"1660320000.jpg\"]\n",
    "results = process_images(image_names, bckg_image)\n",
    "comparison = validation(manual_results, results)\n",
    "\n",
    "for comp in comparison:\n",
    "    mse = np.mean((comp['manual_count'] - comp['detected_count']) ** 2) #sustraendo debe ser la predicción\n",
    "    print(f\"Image: {comp['image']}, Manual Count: {comp['manual_count']}, Detected Count: {comp['detected_count']}, MSE Image level: {mse}, Close Points: {comp['close_points']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d329b69-b6bb-4291-b227-6ea91aad5fae",
   "metadata": {},
   "source": [
    "Notes:  \n",
    "- Currently, the background image keeps with the 2 persons.\n",
    "- They are not equally enlightened in the same way; however, on the test done, the contrast is lowered (in the same image).\n",
    "- Not establishing specific contours for a person (different cases).\n",
    ". How do you achieve accuracy with the points? Establish a certain distance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53adc963-e38f-4fb6-abfb-fc76c18a69bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6b4637-0c48-43f7-8ab5-68edbc0965a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
